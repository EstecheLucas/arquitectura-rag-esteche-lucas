{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9e1d58",
   "metadata": {},
   "source": [
    "# RAG\n",
    "La generación aumentada por recuperación (RAG) es el proceso de optimización de la salida de un modelo de lenguaje de gran tamaño, de modo que haga referencia a una base de conocimientos autorizada fuera de los orígenes de datos de entrenamiento antes de generar una respuesta. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83063d4e",
   "metadata": {},
   "source": [
    "### Instalación de dependencias:\n",
    "\n",
    "```bash\n",
    "uv pip install langchain-community langchain-core langchain-ollama pypdf\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c2bf6",
   "metadata": {},
   "source": [
    "### Cargar los pdfs\n",
    "\n",
    "Recibimos el path del documento pdf por parametro. A partir del documento recibido, lo convertimos en un iterable. \n",
    "Al final, retornamos la variable text con el contenido del documento crudo, sin dividir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d258b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def upload_pdf(url: str):        \n",
    "    try:\n",
    "        loader = PyPDFLoader(url)\n",
    "        loader = loader.lazy_load()\n",
    "\n",
    "        text = \"\"\n",
    "\n",
    "        for page in loader: \n",
    "            text += page.page_content + \"\\n\"\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e842db3d",
   "metadata": {},
   "source": [
    "### text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a767d9",
   "metadata": {},
   "source": [
    "Recibimos el contenido del documento generado en la celda anterior. Ahora parametrizamos `chunk_size` y `chunk_overlap` para poder experimentar con distintos tamaños de fragmento. Como configuración por defecto reducimos los trozos a 1 200 caracteres con 150 de solapamiento para capturar contexto suficiente sin repetir tanto contenido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "653b5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "\n",
    "def text_splitter(\n",
    "    text: str,\n",
    "    *,\n",
    "    chunk_size: int = 1200,\n",
    "    chunk_overlap: int = 150,\n",
    "    separator: str = \"\\n\",\n",
    "    base_metadata: dict | None = None,\n",
    "    report: bool = True,\n",
    ") -> list:\n",
    "\n",
    "    splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separator=separator,\n",
    "    )\n",
    "    base_metadata = base_metadata or {}\n",
    "    texts = splitter.create_documents([text], metadatas=[base_metadata])\n",
    "\n",
    "    fallback_source = base_metadata.get(\"source\", \"uploaded_pdf\")\n",
    "    fallback_topic = base_metadata.get(\"topic\", \"stuxnet\")\n",
    "    for idx, doc in enumerate(texts):\n",
    "        doc.metadata = dict(doc.metadata)\n",
    "        doc.metadata.setdefault(\"source\", fallback_source)\n",
    "        doc.metadata.setdefault(\"topic\", fallback_topic)\n",
    "        doc.metadata[\"chunk_index\"] = idx\n",
    "        normalized = doc.page_content.lower()\n",
    "        doc.metadata[\"mentions_plc\"] = \"plc\" in normalized or \"centrifug\" in normalized\n",
    "        doc.metadata[\"mentions_windows\"] = \"windows\" in normalized or \"siemens\" in normalized\n",
    "\n",
    "    if report and texts:\n",
    "        lengths = [len(doc.page_content) for doc in texts]\n",
    "        avg_len = sum(lengths) / len(lengths)\n",
    "        print(\n",
    "            f\"Generados {len(texts)} chunks | media: {avg_len:.0f} | min: {min(lengths)} | max: {max(lengths)}\"\n",
    "        )\n",
    "    elif report:\n",
    "        print(\"No se generaron chunks. Ajusta los parámetros del splitter.\")\n",
    "\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5f39e",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cafa715",
   "metadata": {},
   "source": [
    "Creamos la conexión con nuestro modelo de embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e4eefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(\n",
    "    model = \"nomic-embed-text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f1e23",
   "metadata": {},
   "source": [
    "### vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458cd3d6",
   "metadata": {},
   "source": [
    "Creamos nuestra vector store con FAISS para mantener colecciones locales que podamos reconstruir, persistir en disco y filtrar mediante metadatos enriquecidos con el splitter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61a54cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "FAISS_INDEX_DIR = Path(\"vectorstore/faiss_langchain\")\n",
    "\n",
    "\n",
    "def _faiss_index_exists(directory: Path) -> bool:\n",
    "    return (directory / \"index.faiss\").exists() and (directory / \"index.pkl\").exists()\n",
    "\n",
    "\n",
    "def load_faiss_vector_store(*, directory: Optional[Path] = None) -> FAISS:\n",
    "    target_dir = Path(directory) if directory else FAISS_INDEX_DIR\n",
    "    if not _faiss_index_exists(target_dir):\n",
    "        raise FileNotFoundError(\n",
    "            f\"No se encontró un índice FAISS en {target_dir}. Ejecuta `upsert_faiss_collection` para generarlo.\"\n",
    "        )\n",
    "    return FAISS.load_local(\n",
    "        str(target_dir),\n",
    "        embeddings=embedding,\n",
    "        allow_dangerous_deserialization=True,  # requerido para deserializar los pickles del índice\n",
    "    )\n",
    "\n",
    "\n",
    "def upsert_faiss_collection(\n",
    "    documents: List[Document],\n",
    "    *,\n",
    "    directory: Optional[Path] = None,\n",
    "    rebuild: bool = False,\n",
    ") -> FAISS:\n",
    "    target_dir = Path(directory) if directory else FAISS_INDEX_DIR\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if rebuild or not _faiss_index_exists(target_dir):\n",
    "        vector_store = FAISS.from_documents(documents, embedding)\n",
    "    else:\n",
    "        vector_store = load_faiss_vector_store(directory=target_dir)\n",
    "        vector_store.add_documents(documents)\n",
    "    vector_store.save_local(str(target_dir))\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def faiss_similarity_search(\n",
    "    query: str,\n",
    "    *,\n",
    "    k: int = 4,\n",
    "    metadata_filter: Optional[Dict[str, Any]] = None,\n",
    "    score_threshold: Optional[float] = None,\n",
    "    directory: Optional[Path] = None,\n",
    "):\n",
    "    vector_store = load_faiss_vector_store(directory=directory)\n",
    "    docs_and_scores = vector_store.similarity_search_with_score(\n",
    "        query,\n",
    "        k=k,\n",
    "        filter=metadata_filter,\n",
    "    )\n",
    "    if score_threshold is not None:\n",
    "        docs_and_scores = [\n",
    "            (doc, score) for doc, score in docs_and_scores if score <= score_threshold\n",
    "        ]\n",
    "    return docs_and_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667e774",
   "metadata": {},
   "source": [
    "#### Gestión práctica del índice FAISS\n",
    "\n",
    "`index_pdf_into_faiss` levanta el pipeline completo (ingesta + enriquecimiento de metadatos) mientras que `preview_faiss_hits` ayuda a evaluar búsquedas con filtros o umbrales antes de llamar al LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7cc52ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def index_pdf_into_faiss(\n",
    "    pdf_path: str,\n",
    "    *,\n",
    "    topic: str = \"stuxnet\",\n",
    "    rebuild: bool = False,\n",
    "):\n",
    "    raw_text = upload_pdf(pdf_path)\n",
    "    documents = text_splitter(\n",
    "        raw_text,\n",
    "        base_metadata={\n",
    "            \"source\": pdf_path,\n",
    "            \"topic\": topic,\n",
    "        },\n",
    "    )\n",
    "    return upsert_faiss_collection(documents, rebuild=rebuild)\n",
    "\n",
    "\n",
    "def preview_faiss_hits(\n",
    "    question: str,\n",
    "    *,\n",
    "    top_k: int = 4,\n",
    "    filter_plc: bool = False,\n",
    "    score_threshold: Optional[float] = None,\n",
    "):\n",
    "    metadata_filter = {\"mentions_plc\": True} if filter_plc else None\n",
    "    docs_and_scores = faiss_similarity_search(\n",
    "        question,\n",
    "        k=top_k,\n",
    "        metadata_filter=metadata_filter,\n",
    "        score_threshold=score_threshold,\n",
    "    )\n",
    "    formatted = []\n",
    "    for doc, score in docs_and_scores:\n",
    "        formatted.append(\n",
    "            {\n",
    "                \"chunk_index\": doc.metadata.get(\"chunk_index\"),\n",
    "                \"source\": doc.metadata.get(\"source\"),\n",
    "                \"topic\": doc.metadata.get(\"topic\"),\n",
    "                \"mentions_plc\": doc.metadata.get(\"mentions_plc\"),\n",
    "                \"score\": round(score, 4),\n",
    "                \"preview\": doc.page_content[:200].strip().replace(\"\\n\", \" \"),\n",
    "            }\n",
    "        )\n",
    "    return formatted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a02e4a",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157f5f9",
   "metadata": {},
   "source": [
    "Creamos una función retriebal, que nos devuelve los documentos en una busqueda de similitudes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8d736bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional\n",
    "\n",
    "\n",
    "def retrieval(\n",
    "    input_user: str,\n",
    "    *,\n",
    "    k: int = 4,\n",
    "    metadata_filter: Optional[Dict[str, Any]] = None,\n",
    "    score_threshold: Optional[float] = None,\n",
    "):\n",
    "    docs_and_scores = faiss_similarity_search(\n",
    "        input_user,\n",
    "        k=k,\n",
    "        metadata_filter=metadata_filter,\n",
    "        score_threshold=score_threshold,\n",
    "    )\n",
    "    return [doc for doc, _ in docs_and_scores]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc850d",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edfabd9",
   "metadata": {},
   "source": [
    "Integramos varias plantillas de prompt parametrizables (tono, idioma y reglas de citación) para experimentar con instrucciones de sistema/humano distintas antes de llamar al LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "baseline_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Eres un asistente especializado en LangChain. Responde en {answer_language} con un tono {style}.\n",
    "- Limítate al contexto proporcionado y reconoce con honestidad cuando falten datos.\n",
    "Contexto verificado:\n",
    "{contexto}\n",
    "\n",
    "Pregunta del usuario:\n",
    "{input_user}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "citation_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Actúas como redactor técnico enfocado en LangChain y debes atribuir cada afirmación.\n",
    "Reglas:\n",
    "1. Prioriza la precisión sobre la cobertura y responde en {answer_language} con un tono {style}.\n",
    "2. Cada frase que use el contexto debe terminar con el marcador [Fuente X] que corresponda al bloque numerado del contexto.\n",
    "3. Si no existe evidencia en el contexto, declara la limitación.\n",
    "Contexto numerado:\n",
    "{contexto}\n",
    "\n",
    "Pregunta:\n",
    "{input_user}\n",
    "\n",
    "Respuesta estructurada:\n",
    "- Conclusión principal\n",
    "- Detalles clave\n",
    "- Riesgos o incógnitas\n",
    "\n",
    "Fuentes consultadas: lista de [Fuente X]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "deliberate_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Eres un analista de ciberseguridad. Responde en {answer_language} con un tono {style}, menciona incertidumbres y cita las fuentes del contexto usando [Fuente X].\",\n",
    "        ),\n",
    "        (\n",
    "            \"ai\",\n",
    "            \"Entendido, validaré la evidencia antes de emitir la respuesta final.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Contexto a citar:\\n{contexto}\\n\\nPregunta del usuario:\\n{input_user}\\n\\n\"\n",
    "            \"Plan: 1) Resume la evidencia clave, 2) Menciona lagunas si existen, \"\n",
    "            \"3) Entrega la respuesta final breve citando [Fuente X].\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PROMPT_VARIANTS = {\n",
    "    \"baseline\": baseline_prompt,\n",
    "    \"citations\": citation_prompt,\n",
    "    \"deliberate_chat\": deliberate_chat_prompt,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def render_prompt(variant: str = \"baseline\", **kwargs):\n",
    "    \"\"\"\n",
    "    Renderiza un prompt normal o de chat según su tipo.\n",
    "    Devuelve:\n",
    "      - texto (str) si es PromptTemplate\n",
    "      - lista de mensajes (list[dict]) si es ChatPromptTemplate\n",
    "    \"\"\"\n",
    "    template = PROMPT_VARIANTS.get(variant)\n",
    "\n",
    "    if template is None:\n",
    "        raise ValueError(f\"Prompt variant desconocido: {variant}\")\n",
    "\n",
    "    \n",
    "    kwargs.setdefault(\"answer_language\", \"es\")\n",
    "    kwargs.setdefault(\"style\", \"directa\")\n",
    "\n",
    "  \n",
    "    if isinstance(template, ChatPromptTemplate):\n",
    "        return template.format_messages(**kwargs)\n",
    "\n",
    "    \n",
    "    return template.format(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536ebce",
   "metadata": {},
   "source": [
    "#### Estrategias de prompting incluidas\n",
    "\n",
    "- `baseline`: respuesta directa y honesta enfocada en preguntas frecuentes.\n",
    "- `citations`: obliga a citar cada afirmación con el marcador `[Fuente X]` generado a partir de los metadatos.\n",
    "- `deliberate_chat`: usa mensajes de sistema/humano para guiar un razonamiento breve antes de entregar la respuesta final.\n",
    "\n",
    "`render_prompt` permite alternar dinámicamente entre estas variantes desde cualquier flujo del cuaderno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9383bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "\n",
    "def format_docs_with_sources(docs: Sequence[Document], *, max_chars: int = 700) -> str:\n",
    "    formatted_blocks = []\n",
    "    for idx, doc in enumerate(docs, start=1):\n",
    "        metadata = getattr(doc, \"metadata\", {}) or {}\n",
    "        source = metadata.get(\"source\", \"desconocido\")\n",
    "        chunk = metadata.get(\"chunk_index\", \"?\")\n",
    "        topic = metadata.get(\"topic\", \"general\")\n",
    "        tags = []\n",
    "        if metadata.get(\"mentions_plc\"):\n",
    "            tags.append(\"PLC\")\n",
    "        if metadata.get(\"mentions_windows\"):\n",
    "            tags.append(\"Windows\")\n",
    "        tag_str = \",\".join(tags) if tags else \"sin-tags\"\n",
    "        snippet = doc.page_content.strip()\n",
    "        if len(snippet) > max_chars:\n",
    "            snippet = snippet[:max_chars].rstrip() + \"...\"\n",
    "        formatted_blocks.append(\n",
    "            f\"[Fuente {idx} | src={source} | chunk={chunk} | topic={topic} | tags={tag_str}]{snippet}\"\n",
    "        )\n",
    "    if not formatted_blocks:\n",
    "        return \"No se encontraron fragmentos relevantes para citar.\"\n",
    "    return \"\\n\".join(formatted_blocks)\n",
    "\n",
    "\n",
    "def ensure_context_text(contexto) -> str:\n",
    "    if isinstance(contexto, str):\n",
    "        return contexto\n",
    "    if isinstance(contexto, Sequence):\n",
    "        return format_docs_with_sources(contexto)\n",
    "    return str(contexto)\n",
    "\n",
    "\n",
    "def preview_prompt_variants(\n",
    "    question: str,\n",
    "    docs: Sequence[Document],\n",
    "    *,\n",
    "    variants: Sequence[str] | None = None,\n",
    "    answer_language: str = \"es\",\n",
    "    style: str = \"directa\",\n",
    "):\n",
    "    context_text = format_docs_with_sources(docs)\n",
    "    variants = variants or list(PROMPT_VARIANTS.keys())\n",
    "    previews = {}\n",
    "    for variant in variants:\n",
    "        rendered = render_prompt(\n",
    "            variant,\n",
    "            contexto=context_text,\n",
    "            input_user=question,\n",
    "            answer_language=answer_language,\n",
    "            style=style,\n",
    "        )\n",
    "        if isinstance(rendered, list):\n",
    "            previews[variant] = \"\".join(\n",
    "                f\"{message.type.upper()}: {message.content}\" for message in rendered\n",
    "            )\n",
    "        else:\n",
    "            previews[variant] = rendered\n",
    "    return previews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafe3c2",
   "metadata": {},
   "source": [
    "### LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f795aa0",
   "metadata": {},
   "source": [
    "Declaramos la función response, responsable de la interacción con el LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "44240362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "\n",
    "def response(\n",
    "    input_user: str,\n",
    "    contexto,\n",
    "    *,\n",
    "    prompt_variant: str = \"baseline\",\n",
    "    answer_language: str = \"es\",\n",
    "    style: str = \"directa\",\n",
    "):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        api_key=api_key,\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    context_text = ensure_context_text(contexto)\n",
    "    rendered_prompt = render_prompt(\n",
    "        prompt_variant,\n",
    "        contexto=context_text,\n",
    "        input_user=input_user,\n",
    "        answer_language=answer_language,\n",
    "        style=style,\n",
    "    )\n",
    "    for chunk in llm.stream(rendered_prompt):\n",
    "        yield chunk.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea202c",
   "metadata": {},
   "source": [
    "Ahora utilizamos las funciones para cargar el documento, aplicar el text_splitter y al final, guardar nuestros datos como embedding en la base de datos vectorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bfddc2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2169, which is longer than the specified 1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generados 18 chunks | media: 1180 | min: 272 | max: 2159\n"
     ]
    }
   ],
   "source": [
    "loader = upload_pdf(\"Stuxnet (1) - copia.pdf\")\n",
    "texts = text_splitter(\n",
    "    loader,\n",
    "    base_metadata={\n",
    "        \"source\": \"Stuxnet (1) - copia.pdf\",\n",
    "        \"topic\": \"stuxnet\",\n",
    "    },\n",
    ")\n",
    "faiss_store = upsert_faiss_collection(texts, rebuild=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d788f8d",
   "metadata": {},
   "source": [
    "## Optimización del Separador de Texto\n",
    "\n",
    "Modifica los parámetros `chunk_size` y `chunk_overlap` para estudiar cómo cambian los documentos resultantes y la calidad del contexto que llega al LLM. La celda siguiente crea una rutina de experimentación que imprime estadísticas de los chunks, los documentos más parecidos a una pregunta y, opcionalmente, la respuesta producida por el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b80e8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2169, which is longer than the specified 800\n",
      "Created a chunk of size 2169, which is longer than the specified 1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size=800, overlap=200 -> 31 chunks (media 780, min 272, max 2159)\n",
      "  1. score=0.574 -> computadoras   de   Asia,   por   lo   que   elaboró   la   hipótesis   sobre   que   se   desarrollaron   varios   modelos   para   atacar ...\n",
      "  2. score=0.562 -> explotar   esta   vulnerabilidad   antes   de   que   se   desarrolle   e   implemente   un   parche   para   solucionarla.   La   falta   d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2169, which is longer than the specified 1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size=1200, overlap=150 -> 18 chunks (media 1179, min 272, max 2159)\n",
      "  1. score=0.551 -> Seminario  de  Actualización  III:   CiberSeguridad   Exposición:  Casos  Reales  Relacionados  a  la   CiberSeguridad   –   Stuxnet     Gru...\n",
      "  2. score=0.545 -> a   la   red   local   y   el   gusano   penetró   así   en   el   sistema   de   la   planta   (Falliere,   Murchu   &   Chien,   2011).   ...\n",
      "chunk_size=1600, overlap=100 -> 14 chunks (media 1433, min 272, max 2159)\n",
      "  1. score=0.559 -> World's   First   Digital   Weapon .   Crown   Publishing   Group.    ●  BBC  News  Mundo.  (2015,  octubre  7).  Stuxnet:  el  virus  infor...\n",
      "  2. score=0.551 -> Seminario  de  Actualización  III:   CiberSeguridad   Exposición:  Casos  Reales  Relacionados  a  la   CiberSeguridad   –   Stuxnet     Gru...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def _normalize_vector(vector):\n",
    "    arr = np.array(vector, dtype=float)\n",
    "    norm = np.linalg.norm(arr)\n",
    "    return arr if norm == 0 else arr / norm\n",
    "\n",
    "\n",
    "def chunking_experiments(\n",
    "    raw_text: str,\n",
    "    question: str,\n",
    "    chunk_trials: List[Dict[str, int]],\n",
    "    top_k: int = 2,\n",
    "    run_llm: bool = False,\n",
    "):\n",
    "    experiment_rows = []\n",
    "\n",
    "    for trial in chunk_trials:\n",
    "        docs = text_splitter(\n",
    "            raw_text,\n",
    "            chunk_size=trial[\"chunk_size\"],\n",
    "            chunk_overlap=trial[\"chunk_overlap\"],\n",
    "            separator=trial.get(\"separator\", \"\\n\"),\n",
    "            report=False,\n",
    "        )\n",
    "\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        stats = {\n",
    "            \"chunks\": len(docs),\n",
    "            \"avg_len\": int(sum(lengths) / len(lengths)) if lengths else 0,\n",
    "            \"min_len\": min(lengths) if lengths else 0,\n",
    "            \"max_len\": max(lengths) if lengths else 0,\n",
    "        }\n",
    "\n",
    "        doc_texts = [doc.page_content for doc in docs]\n",
    "        doc_vectors = [\n",
    "            _normalize_vector(vector)\n",
    "            for vector in embedding.embed_documents(doc_texts)\n",
    "        ]\n",
    "        query_vector = _normalize_vector(embedding.embed_query(question))\n",
    "\n",
    "        scored_docs = sorted(\n",
    "            (\n",
    "                (float(np.dot(query_vector, doc_vector)), doc)\n",
    "                for doc_vector, doc in zip(doc_vectors, docs)\n",
    "            ),\n",
    "            key=lambda item: item[0],\n",
    "            reverse=True,\n",
    "        )[:top_k]\n",
    "\n",
    "        answer = None\n",
    "        best_docs = [doc for _, doc in scored_docs]\n",
    "        if run_llm and best_docs:\n",
    "            context = \"\\n\\n\".join(doc.page_content for doc in best_docs)\n",
    "            answer = \"\".join(response(input_user=question, contexto=context))\n",
    "\n",
    "        summary = (\n",
    "            f\"chunk_size={trial['chunk_size']}, overlap={trial['chunk_overlap']} -> \"\n",
    "            f\"{stats['chunks']} chunks (media {stats['avg_len']}, \"\n",
    "            f\"min {stats['min_len']}, max {stats['max_len']})\"\n",
    "        )\n",
    "        print(summary)\n",
    "        for rank, (score, doc) in enumerate(scored_docs, start=1):\n",
    "            snippet = doc.page_content[:140].replace(\"\\n\", \" \")\n",
    "            print(f\"  {rank}. score={score:.3f} -> {snippet}...\")\n",
    "\n",
    "        if answer:\n",
    "            print(f\"Respuesta del LLM: {answer.strip()}\\n\")\n",
    "\n",
    "        experiment_rows.append(\n",
    "            {\n",
    "                \"params\": trial,\n",
    "                \"stats\": stats,\n",
    "                \"top_docs\": best_docs,\n",
    "                \"answer\": answer,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return experiment_rows\n",
    "\n",
    "\n",
    "chunk_trials = [\n",
    "    {\"chunk_size\": 800, \"chunk_overlap\": 200},\n",
    "    {\"chunk_size\": 1200, \"chunk_overlap\": 150},\n",
    "    {\"chunk_size\": 1600, \"chunk_overlap\": 100},\n",
    "]\n",
    "\n",
    "sample_question = \"¿Cuál es el vector de ataque principal descrito en el informe?\"\n",
    "chunking_summary = chunking_experiments(\n",
    "    loader, sample_question, chunk_trials, top_k=2, run_llm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa3255",
   "metadata": {},
   "source": [
    "### Comparativa de separadores alternativos\n",
    "\n",
    "LangChain ofrece separadores especializados para distintos formatos (texto libre, prompts sensibles al recuento de tokens o documentos con encabezados). La siguiente celda crea un pequeño laboratorio para compararlos y anotar en qué situaciones conviene cada uno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c37e982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RecursiveCharacterTextSplitter\n",
      "  Uso recomendado: Priorizamos saltos de párrafo y frases antes de cortar caracteres sueltos; es ideal para PDF o reportes narrativos.\n",
      "  documentos: 25 | media: 907 | min: 272 | max: 997\n",
      "  ejemplo: Seminario  de  Actualización  III:   CiberSeguridad   Exposición:  Casos  Reales  Relacionados  a  la   CiberSeguridad   –   Stuxnet     Grupo:   Quem...\n",
      "\n",
      "TokenTextSplitter\n",
      "  Uso recomendado: Divide por recuento de tokens (útil para modelos con límites estrictos y textos en varios idiomas).\n",
      "  documentos: 45 | media: 509 | min: 385 | max: 1665\n",
      "  ejemplo: Seminario  de  Actualización  III:   CiberSeguridad   Exposición:  Casos  Reales  Relacionados  a  la   CiberSeguridad   –   Stuxnet     Grupo:   Quem...\n",
      "\n",
      "MarkdownHeaderTextSplitter\n",
      "  Uso recomendado: Respeta jerarquías de encabezados Markdown; perfecto para documentación técnica o actas con secciones claras.\n",
      "  documentos: 0 | media: 0 | min: 0 | max: 0\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "try:\n",
    "    from langchain_text_splitters import TokenTextSplitter\n",
    "except ImportError: \n",
    "    TokenTextSplitter = None\n",
    "\n",
    "\n",
    "def explore_text_splitters(raw_text: str):\n",
    "    markdown_sample = (\n",
    "        \"# Informe de seguridad\"\n",
    "        \"## Ataque\"\n",
    "        \"### Descripción\"\n",
    "        \"Stuxnet combinó múltiples vulnerabilidades de Windows para infiltrarse en las centrifugadoras.\"\n",
    "        \"### Impacto\"\n",
    "        \"Los controladores PLC recibieron cargas manipuladas que modificaron la velocidad de rotación.\"\n",
    "    )\n",
    "\n",
    "    configs = [\n",
    "        {\n",
    "            \"name\": \"RecursiveCharacterTextSplitter\",\n",
    "            \"description\": \"Priorizamos saltos de párrafo y frases antes de cortar caracteres sueltos; es ideal para PDF o reportes narrativos.\",\n",
    "            \"factory\": lambda: RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            ),\n",
    "            \"method\": \"create_documents\",\n",
    "            \"text\": raw_text,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    if TokenTextSplitter is not None:\n",
    "        configs.append(\n",
    "            {\n",
    "                \"name\": \"TokenTextSplitter\",\n",
    "                \"description\": \"Divide por recuento de tokens (útil para modelos con límites estrictos y textos en varios idiomas).\",\n",
    "                \"factory\": lambda: TokenTextSplitter(chunk_size=256, chunk_overlap=40),\n",
    "                \"method\": \"create_documents\",\n",
    "                \"text\": raw_text,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"TokenTextSplitter requiere la dependencia opcional `tiktoken`. Instálala si quieres probarlo.\")\n",
    "\n",
    "    configs.append(\n",
    "        {\n",
    "            \"name\": \"MarkdownHeaderTextSplitter\",\n",
    "            \"description\": \"Respeta jerarquías de encabezados Markdown; perfecto para documentación técnica o actas con secciones claras.\",\n",
    "            \"factory\": lambda: MarkdownHeaderTextSplitter(\n",
    "                headers_to_split_on=[(\"#\", \"titulo\"), (\"##\", \"seccion\"), (\"###\", \"subseccion\")],\n",
    "            ),\n",
    "            \"method\": \"split_text\",\n",
    "            \"text\": markdown_sample,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for cfg in configs:\n",
    "        splitter = cfg[\"factory\"]()\n",
    "        sample_text = cfg[\"text\"]\n",
    "\n",
    "        try:\n",
    "            docs = (\n",
    "                splitter.split_text(sample_text)\n",
    "                if cfg[\"method\"] == \"split_text\"\n",
    "                else splitter.create_documents([sample_text])\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            print(f\"{cfg['name']}: no se pudo procesar -> {exc}\")\n",
    "            continue\n",
    "\n",
    "        lengths = [len(doc.page_content) for doc in docs]\n",
    "        avg_len = int(sum(lengths) / len(lengths)) if lengths else 0\n",
    "        snippet = docs[0].page_content[:150].replace(\"\\n\", \" \") if docs else \"\"\n",
    "        print(\"\")\n",
    "        print(f\"{cfg['name']}\")\n",
    "        print(f\"  Uso recomendado: {cfg['description']}\")\n",
    "        print(\n",
    "            f\"  documentos: {len(docs)} | media: {avg_len} | min: {min(lengths) if lengths else 0} | max: {max(lengths) if lengths else 0}\"\n",
    "        )\n",
    "        if docs:\n",
    "            print(f\"  ejemplo: {snippet}...\")\n",
    "            metadata_preview = getattr(docs[0], 'metadata', {})\n",
    "            if metadata_preview:\n",
    "                print(f\"  metadata ejemplo: {metadata_preview}\")\n",
    "\n",
    "\n",
    "explore_text_splitters(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7c6c1bbac44e1850b329cc0afcd5e",
   "metadata": {},
   "source": [
    "\n",
    "## Exploración de Modelos de Embedding y LLM\n",
    "2. Exploración de Modelos de Embedding y LLM:\n",
    "- Experimenta con diferentes modelos de embedding disponibles en `langchain_ollama` (además de `\"nomic-embed-text\"`) y analiza cómo impactan en la calidad de la recuperación de documentos.\n",
    "- Prueba con otros modelos de lenguaje de gran tamaño (LLM) compatibles con `langchain_google_genai` o `langchain_ollama` y compara sus respuestas y rendimiento.\n",
    "\n",
    "Las celdas siguientes automatizan estas comparativas para que puedas repetirlas fácilmente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02641b89db54f40bc56707166ce1341",
   "metadata": {},
   "source": [
    "\n",
    "### Comparativa rápida de embeddings\n",
    "El bloque de código crea embeddings temporales para cada modelo indicado, calcula la similitud coseno contra varias preguntas y muestra los fragmentos más relevantes. Ajusta la lista `candidate_embedding_models`, las preguntas o el parámetro `top_k` según lo necesites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c83f191bed6429db9c2245bcea8df7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo de embedding: nomic-embed-text\n",
      " Pregunta: ¿Qué es Stuxnet?\n",
      "   1. score=0.744 -> Introducción   En  el  respectivo  trabajo  vamos  a  abordar  el  caso  de  Stuxnet,  uno  de  los   ciberataques   más   reconocidos   y   con   un   impacto   importante   en   ...\n",
      "   2. score=0.731 -> en   constante   cambio   (CiberInseguro,   2022).   El  caso  de  Stuxnet  dejó  en  claro  que  las  guerras  modernas  no  solo  se  libran  con   armas   tradicionales,   sino ...\n",
      " Pregunta: ¿Cómo se propaga?\n",
      "   1. score=0.579 -> que   se   desarrolle   e   implemente   un   parche   para   solucionarla.   La   falta   de   un   parche   disponible   representa   un   riesgo   significativo,   ya   que   lo...\n",
      "   2. score=0.576 -> específicamente   para   atacar   centrifugadoras   Siemens   utilizadas   en   la   planta   nuclear   de   Natanz,   en   Irán,   lo   que   muestra   que   no   se   trataba   d...\n",
      "\n",
      "Modelo de embedding: mxbai-embed-large\n",
      " Pregunta: ¿Qué es Stuxnet?\n",
      "   1. score=0.874 -> una   computadora   a   otra   a   través   de   redes,   como   internet   o   LAN,   sin   necesidad   de   intervención   humana.   A   diferencia   de   otros   tipos   de   ma...\n",
      "   2. score=0.855 -> Introducción   En  el  respectivo  trabajo  vamos  a  abordar  el  caso  de  Stuxnet,  uno  de  los   ciberataques   más   reconocidos   y   con   un   impacto   importante   en   ...\n",
      " Pregunta: ¿Cómo se propaga?\n",
      "   1. score=0.664 -> a   la   red   local   y   el   gusano   penetró   así   en   el   sistema   de   la   planta   (Falliere,   Murchu   &   Chien,   2011).   Con  el  tiempo,  Kaspersky  Lab  encont...\n",
      "   2. score=0.663 -> Seminario  de  Actualización  III:  CiberSeguridad ................................................................. 1 Exposición:  Casos  Reales  Relacionados  a  la  CiberSegurid...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "candidate_embedding_models = [\n",
    "    \"nomic-embed-text\",\n",
    "    \"mxbai-embed-large\"\n",
    "]\n",
    "\n",
    "\n",
    "def _normalize(vector):\n",
    "    arr = np.array(vector, dtype=float)\n",
    "    norm = np.linalg.norm(arr)\n",
    "    return arr if norm == 0 else arr / norm\n",
    "\n",
    "\n",
    "def compare_embedding_models(model_names: List[str], documents: List[Document], questions: List[str], top_k: int = 3) -> Dict:\n",
    "    doc_texts = [doc.page_content for doc in documents]\n",
    "    results = {}\n",
    "\n",
    "    for model_name in model_names:\n",
    "        temp_embedding = OllamaEmbeddings(model=model_name)\n",
    "\n",
    "        doc_vectors = [_normalize(vec) for vec in temp_embedding.embed_documents(doc_texts)]\n",
    "        results[model_name] = {}\n",
    "\n",
    "        for question in questions:\n",
    "            query_vec = _normalize(temp_embedding.embed_query(question))\n",
    "\n",
    "            scored_docs = sorted(\n",
    "                (\n",
    "                    (float(np.dot(query_vec, doc_vec)), doc)\n",
    "                    for doc_vec, doc in zip(doc_vectors, documents)\n",
    "                ),\n",
    "                key=lambda item: item[0],\n",
    "                reverse=True,\n",
    "            )[:top_k]\n",
    "\n",
    "            results[model_name][question] = scored_docs\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def show_embedding_results(results):\n",
    "    for model_name, question_results in results.items():\n",
    "        print(f\"\\nModelo de embedding: {model_name}\")\n",
    "\n",
    "        for question, docs_with_scores in question_results.items():\n",
    "            print(f\" Pregunta: {question}\")\n",
    "\n",
    "            for rank, (score, doc) in enumerate(docs_with_scores, start=1):\n",
    "                snippet = doc.page_content[:180].replace(\"\\n\", \" \")\n",
    "                print(f\"   {rank}. score={score:.3f} -> {snippet}...\")\n",
    "\n",
    "\n",
    "example_questions = [\n",
    "    \"¿Qué es Stuxnet?\",\n",
    "    \"¿Cómo se propaga?\"\n",
    "]\n",
    "\n",
    "embedding_comparisons = compare_embedding_models(\n",
    "    candidate_embedding_models,\n",
    "    texts,              \n",
    "    example_questions,\n",
    "    top_k=2,\n",
    ")\n",
    "\n",
    "show_embedding_results(embedding_comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815170dd8ddf4a9080f527bd9b3a2477",
   "metadata": {},
   "source": [
    "\n",
    "### Comparativa rápida de LLM\n",
    "Con esta celda puedes invocar varios modelos de `langchain_google_genai` o `langchain_ollama` con la misma pregunta y contexto recuperado. Modifica `llm_candidates` para incluir los modelos que tengas disponibles en tu entorno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b548341a7314f65ad635ec0241de7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gemini-2.0-flash\n",
      "----------------\n",
      "El flujo RAG proporciona información sobre el malware Stuxnet, incluyendo su propagación, objetivos, y el uso de vulnerabilidades zero-day.\n",
      "\n",
      "\n",
      "qwen2:1.5b\n",
      "----------\n",
      "El flujo RAG resuelve el problema de la propagación del virus Stuxnet a través de redes informáticas cerradas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Callable, Dict, List, Sequence, Tuple\n",
    "from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "prompt = \"\"\"Usa el siguiente contexto para responder:\n",
    "\n",
    "Contexto:\n",
    "{contexto}\n",
    "\n",
    "Pregunta:\n",
    "{input_user}\n",
    "\n",
    "Respuesta breve y directa:\n",
    "\"\"\"\n",
    "\n",
    "def compare_llms(candidates: Sequence[Tuple[str, Callable[[], object]]], question: str, context_docs: List):\n",
    "    context_text = \"\\n\".join(doc.page_content for doc in context_docs)\n",
    "    answers = {}\n",
    "\n",
    "    for name, factory in candidates:\n",
    "        llm = factory()\n",
    "\n",
    "       \n",
    "        completion = llm.invoke(prompt.format(contexto=context_text, input_user=question))\n",
    "\n",
    "     \n",
    "        content = getattr(completion, \"content\", getattr(completion, \"text\", str(completion)))\n",
    "\n",
    "        answers[name] = content\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "llm_candidates = [\n",
    "    (\"gemini-2.0-flash\", lambda: ChatGoogleGenerativeAI(api_key=api_key, model=\"gemini-2.0-flash\", temperature=0.4)),\n",
    "    (\"qwen2:1.5b\", lambda: ChatOllama(model=\"qwen2:1.5b\", temperature=0.2)),\n",
    "]\n",
    "\n",
    "sample_question = \"Resume brevemente qué problema resuelve este flujo RAG.\"\n",
    "context_docs = retrieval(sample_question)\n",
    "\n",
    "llm_comparisons = compare_llms(llm_candidates, sample_question, context_docs)\n",
    "\n",
    "for name, answer in llm_comparisons.items(): \n",
    "    print(f\"\\n{name}\\n{'-' * len(name)}\\n{answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "225d5b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fuente 1 | src=Stuxnet (1) - copia.pdf | chunk=12 | topic=stuxnet | tags=Windows]en\n",
      " \n",
      "constante\n",
      " \n",
      "cambio\n",
      " \n",
      "(CiberInseguro,\n",
      " \n",
      "2022).\n",
      " \n",
      "El  caso  de  Stuxnet  dejó  en  claro  que  las  guerras  modernas  no  solo  se  libran  con  \n",
      "armas\n",
      " \n",
      "tradicionales,\n",
      " \n",
      "sino\n",
      " \n",
      "también\n",
      " \n",
      "en\n",
      " \n",
      "el\n",
      " \n",
      "terreno\n",
      " \n",
      "digital.\n",
      " \n",
      "Este\n",
      " \n",
      "malware\n",
      " \n",
      "marcó\n",
      " \n",
      "un\n",
      " \n",
      "antes\n",
      " \n",
      "y\n",
      " \n",
      "un\n",
      " \n",
      "después\n",
      " \n",
      "porque\n",
      " \n",
      "demostró\n",
      " \n",
      "que\n",
      " \n",
      "era\n",
      " \n",
      "posible\n",
      " \n",
      "dañar\n",
      " \n",
      "infraestructuras\n",
      " \n",
      "críticas\n",
      " \n",
      "de\n",
      " \n",
      "forma\n",
      " \n",
      "remota\n",
      " \n",
      "y\n",
      " \n",
      "sin\n",
      " \n",
      "intervención\n",
      " \n",
      "directa\n",
      " \n",
      "en\n",
      " \n",
      "el\n",
      " \n",
      "terreno.\n",
      "Stuxnet  salió  de  Irán  \n",
      "La  forma  en  que  Stuxnet  salió  de  Irán  fue  mediante  computadoras  de  contratistas,  \n",
      "memorias\n",
      " \n",
      "USB\n",
      " \n",
      "y\n",
      " \n",
      "redes\n",
      " \n",
      "de\n",
      " \n",
      "empresas\n",
      " \n",
      "multinacionales.\n",
      " \n",
      "Se\n",
      " \n",
      "detectaron\n",
      " \n",
      "infecciones\n",
      " \n",
      "en\n",
      " \n",
      "diferentes\n",
      " \n",
      "países:\n",
      " \n",
      "India  e  Indonesi...\n",
      "[Fuente 2 | src=Stuxnet (1) - copia.pdf | chunk=2 | topic=stuxnet | tags=sin-tags]Introducción   En  el  respectivo  trabajo  vamos  a  abordar  el  caso  de  Stuxnet,  uno  de  los  \n",
      "ciberataques\n",
      " \n",
      "más\n",
      " \n",
      "reconocidos\n",
      " \n",
      "y\n",
      " \n",
      "con\n",
      " \n",
      "un\n",
      " \n",
      "impacto\n",
      " \n",
      "importante\n",
      " \n",
      "en\n",
      " \n",
      "la\n",
      " \n",
      "historia,\n",
      " \n",
      "el\n",
      " \n",
      "cual\n",
      " \n",
      "se\n",
      " \n",
      "considera\n",
      " \n",
      "como\n",
      " \n",
      "punto\n",
      " \n",
      "de\n",
      " \n",
      "inflexión\n",
      " \n",
      "en\n",
      " \n",
      "la\n",
      " \n",
      "ciberseguridad\n",
      " \n",
      "mundial.\n",
      " \n",
      "A\n",
      " \n",
      "lo\n",
      " \n",
      "largo\n",
      " \n",
      "del\n",
      " \n",
      "texto\n",
      " \n",
      "vamos\n",
      " \n",
      "a\n",
      " \n",
      "contar,\n",
      " \n",
      "qué\n",
      " \n",
      "fue\n",
      " \n",
      "Stuxnet,\n",
      " \n",
      "el\n",
      " \n",
      "contexto\n",
      " \n",
      "en\n",
      " \n",
      "que\n",
      " \n",
      "surge,\n",
      " \n",
      "las\n",
      " \n",
      "vulnerabilidades\n",
      " \n",
      "zero-day\n",
      " \n",
      "que\n",
      " \n",
      "explotó\n",
      " \n",
      "y\n",
      " \n",
      "su\n",
      " \n",
      "impacto\n",
      " \n",
      "en\n",
      " \n",
      "las\n",
      " \n",
      "infraestructuras\n",
      " \n",
      "industriales.\n",
      " \n",
      "También\n",
      " \n",
      "vamos\n",
      " \n",
      "a\n",
      " \n",
      "explorar\n",
      " \n",
      "su\n",
      " \n",
      "influencia\n",
      " \n",
      "en\n",
      " \n",
      "la\n",
      " \n",
      "evolución\n",
      " \n",
      "y\n",
      " \n",
      "desarrollo\n",
      " \n",
      "en\n",
      " \n",
      "la\n",
      " \n",
      "ciberseguridad\n",
      " \n",
      "global\n",
      " \n",
      "y\n",
      " \n",
      "el\n",
      " \n",
      "impacto\n",
      " \n",
      "en\n",
      " \n",
      "el\n",
      " \n",
      "ámbito\n",
      " \n",
      "político,...\n",
      "[Fuente 3 | src=Stuxnet (1) - copia.pdf | chunk=3 | topic=stuxnet | tags=PLC]una\n",
      " \n",
      "computadora\n",
      " \n",
      "a\n",
      " \n",
      "otra\n",
      " \n",
      "a\n",
      " \n",
      "través\n",
      " \n",
      "de\n",
      " \n",
      "redes,\n",
      " \n",
      "como\n",
      " \n",
      "internet\n",
      " \n",
      "o\n",
      " \n",
      "LAN,\n",
      " \n",
      "sin\n",
      " \n",
      "necesidad\n",
      " \n",
      "de\n",
      " \n",
      "intervención\n",
      " \n",
      "humana.\n",
      " \n",
      "A\n",
      " \n",
      "diferencia\n",
      " \n",
      "de\n",
      " \n",
      "otros\n",
      " \n",
      "tipos\n",
      " \n",
      "de\n",
      " \n",
      "malware,\n",
      " \n",
      "los\n",
      " \n",
      "gusanos\n",
      " \n",
      "no\n",
      " \n",
      "suelen\n",
      " \n",
      "infectar\n",
      " \n",
      "archivos\n",
      " \n",
      "en\n",
      " \n",
      "el\n",
      " \n",
      "ordenador,\n",
      " \n",
      "sino\n",
      " \n",
      "que\n",
      " \n",
      "se\n",
      " \n",
      "replican\n",
      " \n",
      "y\n",
      " \n",
      "envían\n",
      " \n",
      "copias\n",
      " \n",
      "de\n",
      " \n",
      "sí\n",
      " \n",
      "mismos\n",
      " \n",
      "a\n",
      " \n",
      "otros\n",
      " \n",
      "dispositivos\n",
      " \n",
      "en\n",
      " \n",
      "la\n",
      " \n",
      "red.\n",
      " \n",
      "Esto\n",
      " \n",
      "los\n",
      " \n",
      "convierte\n",
      " \n",
      "en\n",
      " \n",
      "una\n",
      " \n",
      "amenaza\n",
      " \n",
      "significativa,\n",
      " \n",
      "ya\n",
      " \n",
      "que\n",
      " \n",
      "pueden\n",
      " \n",
      "causar\n",
      " \n",
      "daños\n",
      " \n",
      "extensos\n",
      " \n",
      "y\n",
      " \n",
      "congestionar\n",
      " \n",
      "redes\n",
      " \n",
      "(Falliere,\n",
      " \n",
      "Murchu\n",
      " \n",
      "&\n",
      " \n",
      "Chien,\n",
      " \n",
      "2011).\n",
      " \n",
      "Diferencia  entre  Gusanos  Informáticos  y  otros  virus\n",
      "¿Qué  fue  Stuxnet?   Stuxnet  fue  un  gusano  informático  diseñado  para  ataca...\n",
      "[Fuente 4 | src=Stuxnet (1) - copia.pdf | chunk=13 | topic=stuxnet | tags=Windows]también\n",
      " \n",
      "se\n",
      " \n",
      "infectaron.\n",
      " \n",
      "En  total,  según  Symantec,  alrededor  del  60%  de  las  infecciones  se  dieron  en  Irán,  y  \n",
      "el\n",
      " \n",
      "40%\n",
      " \n",
      "se\n",
      " \n",
      "distribuyó\n",
      " \n",
      "en\n",
      " \n",
      "el\n",
      " \n",
      "resto\n",
      " \n",
      "del\n",
      " \n",
      "mundo.\n",
      " \n",
      "Como  Stuxnet  fue  detenido  en  Israel  \n",
      "Ingenieros  iraníes,  con  ayuda  de  Siemens  y  expertos  internacionales,  desarrollaron  \n",
      "procesos\n",
      " \n",
      "de\n",
      " \n",
      "limpieza\n",
      " \n",
      "de\n",
      " \n",
      "los\n",
      " \n",
      "sistemas\n",
      " \n",
      "infectados.\n",
      " \n",
      "Se\n",
      " \n",
      "aplicaron\n",
      " \n",
      "parches\n",
      " \n",
      "de\n",
      " \n",
      "seguridad\n",
      " \n",
      "en\n",
      " \n",
      "Windows\n",
      " \n",
      "y\n",
      " \n",
      "actualizaciones\n",
      " \n",
      "en\n",
      " \n",
      "el\n",
      " \n",
      "software\n",
      " \n",
      "de\n",
      " \n",
      "control.Siemens\n",
      " \n",
      "lanzó\n",
      " \n",
      "herramientas\n",
      " \n",
      "de\n",
      " \n",
      "eliminación\n",
      " \n",
      "con\n",
      " \n",
      "utilidad\n",
      " \n",
      "para\n",
      " \n",
      "detectar\n",
      " \n",
      "y\n",
      " \n",
      "remover\n",
      " \n",
      "Stuxnet\n",
      " \n",
      "de\n",
      " \n",
      "los\n",
      " \n",
      "controladores\n",
      " \n",
      "Step7.Gracias\n",
      " \n",
      "a\n",
      " \n",
      "esto\n",
      " \n",
      "en\n",
      " \n",
      "febrero\n",
      " \n",
      "de\n",
      " \n",
      "2012\n",
      " \n",
      "Ir...\n"
     ]
    }
   ],
   "source": [
    "print(format_docs_with_sources(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a7b76",
   "metadata": {},
   "source": [
    "Interactuamos con nuestro RAG aplicando filtros opcionales y escogiendo la variante de prompt que mejor se adapte al estilo deseado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "22430466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contexto preparado (previsualización):\n",
      "\n",
      "[Fuente 1 | src=Stuxnet (1) - copia.pdf | chunk=12 | topic=stuxnet | tags=Windows]en\n",
      " \n",
      "constante\n",
      " \n",
      "cambio\n",
      " \n",
      "(CiberInseguro,\n",
      " \n",
      "2022).\n",
      " \n",
      "El  caso  de  Stuxnet  dejó  en  claro  que  las  guerras  modernas  no  solo  se  libran  con  \n",
      "armas\n",
      " \n",
      "tradicionales,\n",
      " \n",
      "sino\n",
      " \n",
      "también\n",
      " \n",
      "en\n",
      " \n",
      "el\n",
      " \n",
      "terreno\n",
      " \n",
      "digital.\n",
      " \n",
      "Este\n",
      " \n",
      "malware\n",
      " \n",
      "marcó\n",
      " \n",
      "un\n",
      " \n",
      "antes\n",
      " \n",
      "y\n",
      " \n",
      "un\n",
      " \n",
      "después\n",
      " \n",
      "porque\n",
      " \n",
      "demostró\n",
      " \n",
      "que\n",
      " \n",
      "era\n",
      " \n",
      "posible\n",
      " \n",
      "dañar\n",
      " \n",
      "infraestructuras\n",
      " \n",
      "críticas\n",
      " \n",
      "de\n",
      " \n",
      "forma\n",
      " \n",
      "remota\n",
      " \n",
      "y\n",
      " \n",
      "sin\n",
      " \n",
      "intervención\n",
      " \n",
      "directa\n",
      " \n",
      "en\n",
      " \n",
      "el\n",
      " \n",
      "terreno.\n",
      "Stuxnet  salió  de  Irán  \n",
      "La  forma  en  que  Stuxnet  salió  de  Irán  fue  mediante  computadoras  de  contratistas,  \n",
      "memorias\n",
      " \n",
      "USB\n",
      " \n",
      "y\n",
      " \n",
      "redes\n",
      " \n",
      "de\n",
      " \n",
      "empresas\n",
      " \n",
      "multinacionales.\n",
      " \n",
      "Se\n",
      " \n",
      "detectaron\n",
      " \n",
      "infecciones\n",
      " \n",
      "en\n",
      " \n",
      "diferentes\n",
      " \n",
      "países:\n",
      " \n",
      "India  e  Indonesi...\n",
      "[Fuente 2 | src=Stuxnet (1) - copia.pdf | chunk=2 | topic=stuxnet | tags=sin-tags]Introducción   En  el  respectivo  trabajo  vamos  a  abordar  el  caso  de  Stuxnet,  uno  de  los  \n",
      "ciberataques\n",
      " \n",
      "más\n",
      " \n",
      "reconocidos\n",
      " \n",
      "y\n",
      " \n",
      "con\n",
      " \n",
      "un\n",
      " \n",
      "impacto\n",
      " \n",
      "importante\n",
      " \n",
      "en\n",
      " \n",
      "la\n",
      " \n",
      "historia,\n",
      " \n",
      "el\n",
      " \n",
      "cual\n",
      " \n",
      "se\n",
      " \n",
      "considera\n",
      " \n",
      "como\n",
      " \n",
      "punto\n",
      " \n",
      "de\n",
      " \n",
      "inflexión\n",
      " \n",
      "en\n",
      " \n",
      "la\n",
      " \n",
      "ciberseguridad\n",
      " \n",
      "mundial.\n",
      " \n",
      "A\n",
      " \n",
      "lo\n",
      " \n",
      "largo\n",
      " \n",
      "del\n",
      " \n",
      "texto\n",
      " \n",
      "vamos...\n",
      "\n",
      "Respuesta del LLM:\n",
      "\n",
      "Stuxnet fue un gusano informático diseñado para atacar [Fuente 3].\n",
      "\n",
      "Stuxnet es considerado un punto de inflexión en la ciberseguridad mundial [Fuente 2]. Este malware demostró la posibilidad de dañar infraestructuras críticas de forma remota y sin intervención directa [Fuente 1]. Stuxnet se propagó fuera de Irán a través de computadoras de contratistas, memorias USB y redes de empresas multinacionales [Fuente 1].\n",
      "\n",
      "No se identifican riesgos o incógnitas en el contexto proporcionado.\n",
      "\n",
      "Fuentes consultadas: [Fuente 3, Fuente 2, Fuente 1]\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Pregunta del humano: \").strip()\n",
    "if not question:\n",
    "    raise ValueError(\"Debes ingresar una pregunta para continuar.\")\n",
    "\n",
    "variant_hint = \", \".join(PROMPT_VARIANTS.keys())\n",
    "chosen_variant = input(\n",
    "    f\"Elige prompt variant ({variant_hint}) [citations]: \"\n",
    ").strip().lower() or \"citations\"\n",
    "\n",
    "if chosen_variant not in PROMPT_VARIANTS:\n",
    "    print(\"Variante desconocida, usando 'citations'.\")\n",
    "    chosen_variant = \"citations\"\n",
    "\n",
    "answer_language = input(\"Idioma de respuesta [es]: \").strip() or \"es\"\n",
    "tone = input(\"Tono deseado [directa]: \").strip() or \"directa\"\n",
    "\n",
    "docs = retrieval(question)\n",
    "context_block = format_docs_with_sources(docs)\n",
    "\n",
    "preview = (\n",
    "    context_block\n",
    "    if len(context_block) <= 1200\n",
    "    else context_block[:1200] + \"...\"\n",
    ")\n",
    "\n",
    "print(\"\\nContexto preparado (previsualización):\\n\")\n",
    "print(preview)\n",
    "print(\"\\nRespuesta del LLM:\\n\")\n",
    "\n",
    "for chunk in response(\n",
    "    input_user=question,\n",
    "    contexto=context_block,\n",
    "    prompt_variant=chosen_variant,\n",
    "    answer_language=answer_language,\n",
    "    style=tone,\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
